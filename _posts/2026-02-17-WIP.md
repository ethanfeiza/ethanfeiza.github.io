---
title: "WIP... Exploring Markov Chains"
date: 2026-02-17
tags: [Stochastic Modeling, Linear Algebra, Python]
excerpt: "Explore the Use of Markov Chains in Stochastic Modeling"
---

## Background
When building a discrete predictive model, it’s natural to assume that adding more features will improve performance. Complex neural networks in particular seem to demand large feature sets and deep historical data to generate accurate predictions. However, high performance doesn’t always require that degree of complexity.

A *Stochastic Process* describes a system where state changes are driven by probability. This type of process follows an assumption known as the *Markov Property* - the next state of the system depends only on the current state, not the sequence of prior states that led to the current state. This idea forms a core framework in probabilistic modeling.

A *First-Order Markov Chain* is a mathematical model that uses this property to predict the next state of a system given only its current state. Despite the simplicity in requiring no memory of past states, this approach can prove powerful across many applications, like natural language processing or predicting how users navigate websites.

## The Basics of a Markov Chain

Before we look at a real-world application of a Markov chain model, let’s familiarize ourselves with some basic concepts. For any stochastic process, we must define the *State Space* S: a complete set of states covering all possible system outcomes. For example, let’s define a system with exactly three states. Then the state space S = {S<sub>1</sub>, S<sub>2</sub>, S<sub>3</sub>}. From any state S<sub>n</sub> in S, the system can transition to one of the other two states, or remain in the same state.

![]({{ site.url }}{{ site.baseurl }}/images/MarkovChain/MarkovChainStructure.png)<!-- -->

A transition from state i to state j is written as *i*→*j*. Consider a random state in our system like S<sub>1</sub>. From S<sub>1</sub>, there exist three possible transitions:

**S<sub>1</sub>→S<sub>1</sub>** : starting from state S<sub>1</sub>, we stay in state S<sub>1</sub>.  
**S<sub>1</sub>→S<sub>2</sub>** : starting from state S<sub>1</sub>, we transition to state S<sub>2</sub>.  
**S<sub>1</sub>→S<sub>3</sub>** : starting from state S<sub>1</sub>, we transition to state S<sub>3</sub>.  

![]({{ site.url }}{{ site.baseurl }}/images/MarkovChain/TransitionSet.png)<!-- -->

Each transition *i*→*j* has a *transition probability* - the likelihood of transitioning from state *i* to state *j*. The transition probability P<sub>ij</sub> can be written as:

$$
\normalsize
P_{ij}
\;=\;
P(S_i \rightarrow S_j)
\;=\;
P\!\left(S_{t+1} = S_j \mid S_t = S_i\right),
\qquad
t \text{ = state step}
$$

In order to estimate the probability of a transition occuring, P&#770;<sub>ij</sub>, we must first observe the system and count the number of transitions from *i*→*j* that occur. These observable counts are denoted c<sub>ij</sub>.

![]({{ site.url }}{{ site.baseurl }}/images/MarkovChain/TransitionCounts.png)<!-- -->

We'll also need to count the number of transitions from *i*→*k* that have occurred, where *k* denotes any possible transition state from *i*. We can denote the total count of transitions from *i* as c<sub>ik</sub>. Now, we're ready to estimate the transition probability P&#770;<sub>ij</sub>. The maximum likelihood estimator for Markov chain transition probabilities is:

$$
\normalsize \hat{P}_{ij} = \frac{c_{ij}}{\sum_{k} c_{ik}}
$$

Once calculating P&#770;<sub>ij</sub> for every possible transition in the system, we can update our original diagram with transition probabilities.

![]({{ site.url }}{{ site.baseurl }}/images/MarkovChain/TransitionProbabilities.png)<!-- -->

Note: The transition probabilities from any state add up to one, ensuring all possible next states are accounted for.

## Application

Markov chain models are useful when a system's future state depends primarily on its current state rather than its full history. Compared to more complex models, they require fewer features (and fewer assumptions about their correlations) while maintaining interpretability of state-to-state dynamics. 

Let's dive into a real use case: modeling user navigation on an online retailer website. As web users explore a retail site, they navigate through links on their way to completing a purchase. These interactions might be captured via clickstream logs or server-side analytics services. I simulated a dataset capturing 100,000 individual user sessions on a fictitious retail website: Ethan's Marketplace (kudos for originality). Here's a visual of our website's homepage highlighting all possible user navigations (states) in this dataset.

<iframe 
  src="{{ site.url }}{{ site.baseurl }}/images/MarkovChain/homepage_markov.html"
  width="1280" 
  height="500" 
  style="border:none; max-width:100%;"
  scrolling="no">
</iframe>
<p style="font-size: 13px; color: #999; margin-top: 6px;">UI design generated by Claude Sonnet 4.6</p>

By defintion, every user journey will start on one of three pages: the homepage (95% of sessions), the search page (3%), or the customer support page (2%). Our simulated dataset contains only three columns.

**session_id** is the unique ID for each user session.<br>
**t** is the click number from the start of a user session.<br>
**state** is the web page the user navigates to.<br>

Here's an example of a single user session from the dataset.

<div style="max-width: 400px;">
<table>
  <thead>
    <tr>
      <th>session_id</th>
      <th>t</th>
      <th>state</th>
    </tr>
  </thead>
  <tbody>
    <tr><td>6</td><td>0</td><td>home</td></tr>
    <tr><td>6</td><td>1</td><td>search</td></tr>
    <tr><td>6</td><td>2</td><td>category</td></tr>
    <tr><td>6</td><td>3</td><td>product</td></tr>
    <tr><td>6</td><td>4</td><td>cart</td></tr>
    <tr><td>6</td><td>5</td><td>Checkout</td></tr>
  </tbody>
</table>
</div>

Now, let's develop a Markov chain model to answer some questions regarding user behavior on our website.

## Modeling

The first step in building our Markov chain will be identifying all transient and absorbing states from our state set S.

**Transient States** are states that the process can leave and possibly return to later.<br>
**Absorbing States** are states that cannot be left. Once entered, it is impossible for the process to transition to another state.

In our model, reaching the checkout page will mark the end of a user session. Similarly, if a user leaves the site for any reason, their session will terminate immediately. Thus, we define two absorbing states in S: *Checkout* and *Bounce*. All remaining states will be defined as transient, since the user can navigate back to these states at any time. We'll use this information to modifying the dataset slightly, creating state navigation paths (*i*→*j*) from each row. This will help us with counting individual transitions between states. 

```python
# Loading in our data...
df = pd.read_csv("userSessionLog.csv")

# Order dataset by session_id, then click event (this will help us define the 'next state' within row context)
df = df.sort_values(["session_id", "t"])

# Define states (pages) on the website
states = df['state'].unique().tolist()

# Create 'next state' value in the row
df["next_state"] = df.groupby("session_id")["state"].shift(-1)

# Drop rows that are final events (next state is null)
df = df.dropna(subset=['next_state'])

# Display cleaned dataset
df.display()
```

<div style="max-width: 500px;">
<table>
<thead>
<tr>
<th>session_id</th>
<th>t</th>
<th>state</th>
<th>next_state</th>
</tr>
</thead>
<tbody>
<tr><td>0</td><td>0</td><td>home</td><td>search</td></tr>
<tr><td>0</td><td>1</td><td>search</td><td>category</td></tr>
<tr><td>0</td><td>2</td><td>category</td><td>category</td></tr>
<tr><td>0</td><td>3</td><td>category</td><td>category</td></tr>
<tr><td>0</td><td>4</td><td>category</td><td>bounce</td></tr>
<tr><td>1</td><td>0</td><td>home</td><td>search</td></tr>
</tbody>
</table>
</div>

Next, we'll define a *Transition Probability Matrix* for the Markov chain using counts of state transitions in our dataset. Think of this as an organized way of counting the observed transitions between states.

```python
# Define the number of states in the model (8 in total)
n = len(states)

# Create an 8x8 array to count unique i->j state transitions from the dataset
counts = np.zeros((n, n), dtype=float)

# Map each state to an integer
S2I = {}
for i, s in enumerate(states):
    S2I[s] = i

# Using two arrays: from-states & to-states...
for s_from, s_to in zip(df["state"].to_numpy(), df["next_state"].to_numpy()):

    # ...Update counts of transitions from i to j in the counts matrix
    counts[S2I[s_from], S2I[s_to]] += 1.0

# Create counts as a dataframe
counts_df = pd.DataFrame(counts, index=states, columns=states)

# View the individual counts by i->j state transition
counts_df.display()
```

<div style="max-width: 900px; overflow-x: auto;">
<table border="1" cellspacing="0" cellpadding="4">
  <thead>
    <tr>
      <th></th>
      <th>home</th>
      <th>search</th>
      <th>category</th>
      <th>bounce</th>
      <th>product</th>
      <th>cart</th>
      <th>support</th>
      <th>checkout</th>
    </tr>
  </thead>
  <tbody>
    <tr><th>home</th><td>6,186</td><td>27,404</td><td>22,507</td><td>31,100</td><td>22,332</td><td>8,762</td><td>6,123</td><td>0</td></tr>
    <tr><th>search</th><td>3,298</td><td>3,364</td><td>13,093</td><td>11,804</td><td>13,108</td><td>0</td><td>3,335</td><td>0</td></tr>
    <tr><th>category</th><td>3,459</td><td>3,440</td><td>3,972</td><td>11,068</td><td>20,921</td><td>5,033</td><td>2,042</td><td>0</td></tr>
    <tr><th>bounce</th><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
    <tr><th>product</th><td>8,480</td><td>7,131</td><td>8,560</td><td>17,689</td><td>7,167</td><td>17,828</td><td>4,314</td><td>0</td></tr>
    <tr><th>cart</th><td>988</td><td>0</td><td>0</td><td>5,701</td><td>5,933</td><td>2,729</td><td>2,503</td><td>16,498</td></tr>
    <tr><th>support</th><td>6,891</td><td>3,739</td><td>1,803</td><td>6,140</td><td>1,708</td><td>0</td><td>1,466</td><td>0</td></tr>
    <tr><th>checkout</th><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
  </tbody>
</table>
</div>

From each state i (row) to state j (column), we observe these transition counts from the dataset. Observe that no transitions originate from either absorbing state — this is to be expected. Next, we'll estimate transition probabilities using these counts for each state-to-state transition. Recall the formula for estimating the transition probability P&#770;<sub>ij</sub>:

$$
\normalsize \hat{P}_{ij} = \frac{c_{ij}}{\sum_{k} c_{ik}}
$$

Keep in mind our probability estimates are only as reliable as the data behind them. Sure, we know definitively that transitions *from absorbing states* will always be zero. But what about the several instances above where we observe 0 transitions between transient states? No transitions exist *from* several states above, like cart → category. That doesn’t necessarily mean the move is impossible, just very unlikely. To avoid hard-coding impossibility, we can introduce a concept known as Laplace smoothing. This adds a small constant to every transition count from transient states, ensuring no probability is zero and all rows sum to one. This stabilizes estimates when some transitions are missing in the data.

\[
\hat{P}_{ij} = \frac{c_{ij} + \alpha}{\sum_k (c_{ik} + \alpha)}, \quad \text{where } i \text{ is a transient state}
\]

We'll select an alpha value of 0.1 for smoothing. Now, time to build our transition probability matrix.

```python
# Define a small alpha for Laplace smoothing 
alpha = 0.1

# Smooth counts for each transition i->j
smoothed_counts_df = counts_df + alpha 

# Recalculate i->j counts by row
smoothed_row_totals = smoothed_counts_df.sum(axis=1)

# Renormalize rows in the transition probability matrix to get final estimate
P_hat_df = smoothed_counts_df.div(smoothed_row_totals, axis=0)
```
