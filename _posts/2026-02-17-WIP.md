---
title: "WIP... Exploring Markov Chains"
date: 2026-02-17
tags: [Stochastic Modeling, Linear Algebra, Python]
excerpt: "Explore the Use of Markov Chains in Stochastic Modeling"
---

## Background
When building a discrete predictive model, it’s natural to assume that adding more features will improve performance. Complex neural networks in particular seem to demand large feature sets and deep historical data to generate accurate predictions. However, high performance doesn’t always require that degree of complexity.

A *Stochastic Process* describes a system where state changes are driven by probability. This type of process follows an assumption known as the *Markov Property* - the next state of the system depends only on the current state, not the sequence of prior states that led to the current state. This idea forms a core framework in probabilistic modeling.

A *First-Order Markov Chain* is a mathematical model that uses this property to predict the next state of a system given only its current state. Despite the simplicity in requiring no memory of past states, this approach can prove powerful across many applications, like natural language processing or predicting how users navigate websites.

## The Basics of a Markov Chain

Before we look at a real-world application of a Markov chain model, let’s familiarize ourselves with some basic concepts. For any stochastic process, we must define the *State Space* S: a complete set of states covering all possible system outcomes. For example, let’s define a system with exactly three states. Then the state space S = {S<sub>1</sub>, S<sub>2</sub>, S<sub>3</sub>}. From any state S<sub>n</sub> in S, the system can transition to one of the other two states, or remain in the same state.

![]({{ site.url }}{{ site.baseurl }}/images/MarkovChain/MarkovChainStructure.png)<!-- -->

A transition from state i to state j is written as *i*→*j*. Consider a random state in our system like S<sub>1</sub>. From S<sub>1</sub>, there exist three possible transitions:

**S<sub>1</sub>→S<sub>1</sub>** : starting from state S<sub>1</sub>, we stay in state S<sub>1</sub>.  
**S<sub>1</sub>→S<sub>2</sub>** : starting from state S<sub>1</sub>, we transition to state S<sub>2</sub>.  
**S<sub>1</sub>→S<sub>3</sub>** : starting from state S<sub>1</sub>, we transition to state S<sub>3</sub>.  

![]({{ site.url }}{{ site.baseurl }}/images/MarkovChain/TransitionSet.png)<!-- -->

Each transition *i*→*j* has a *transition probability* - the likelihood of transitioning from state *i* to state *j*. The transition probability P<sub>ij</sub> can be written as:<br>
<br>
**P<sub>ij</sub> = P(S<sub>i</sub>→S<sub>j</sub>) = P(S<sub>t+1</sub> = S<sub>j</sub> | S<sub>t</sub> = S<sub>i</sub>)** where *t* indicates a state-step in the system.<br>

In order to estimate the probability of a transition occuring, P&#770;<sub>ij</sub>, we must first observe the system and count the number of transitions from *i*→*j* that occur. These observable counts are denoted c<sub>ij</sub>.

![]({{ site.url }}{{ site.baseurl }}/images/MarkovChain/TransitionCounts.png)<!-- -->

We'll also need to count the number of transitions from *i*→*k* that have occurred, where *k* denotes any possible transition state from *i*. We can denote the total count of transitions from *i* as c<sub>ik</sub>. Now, we're ready to estimate the transition probability P&#770;<sub>ij</sub>. The maximum likelihood estimator for Markov chain transition probabilities is:
<br>
<br>
**P&#770;<sub>ij</sub> = c<sub>ij</sub> / &sum;<sub>k</sub> c<sub>ik</sub>**

Once calculating P&#770;<sub>ij</sub> for every possible transition in the system, we can update our original diagram with transition probabilities.

![]({{ site.url }}{{ site.baseurl }}/images/MarkovChain/TransitionProbabilities.png)<!-- -->

Note: The transition probabilities from any state add up to one, ensuring all possible next states are accounted for.

## Application

To recap, Markov chain models are useful when a system's future state depends primarily on its current state rather than its full history. Compared to more complex models, they require fewer features (and fewer assumptions about their correlations) while maintaining interpretability of state-to-state dynamics. 

Let's dive into a real use case - modeling user navigation on an online retailer website. Here is a visual of the website's homepage highlighting possible user navigations (states) for the Markov chain.

<iframe 
  src="{{ site.url }}{{ site.baseurl }}/images/MarkovChain/homepage_markov.html"
  width="1280" 
  height="500" 
  style="border:none; max-width:100%;"
  scrolling="no">
</iframe>
<p style="font-size: 12px; color: #999; margin-top: 6px;">UI design generated by Claude Sonnet 4.6</p>

I simulated a dataset capturing 10,000 individual user sessions on this website. By defintion, every user journey will start on one of three pages: the homepage (95% of sessions), the search page (3%), or the customer support page (2%). The dataset contains three columns.<br><br>
**session_id** is the unique ID for a user session.<br>
**t** is the click number from the start of the user session.<br>
**state** is the web page that the user lands on.<br>

