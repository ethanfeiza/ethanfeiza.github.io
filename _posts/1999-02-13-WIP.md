---
title: "WIP... Exploring Markov Chains"
date: 1999-02-13
tags: [Stochastic Modeling, Linear Algebra, Python]
excerpt: "Explore the Therory of Markov Chains in Stochastic Modeling"
---

## Background
When creating a discrete predictive model, intuition may lead us to believe that more features equals stronger results. A powerful neural network might require an extensive feature set and rich history to predict an accurate output. However, this is not always necessary to achieve the desired result.

A *Stochastic Process* describes a system where state changes are driven by probability. This process follows an assumption known as the *Markov Property*: the next state of the system depends only on the current state, not the sequence of prior states that led to the current. This idea forms a core framework in probabilistic modeling.

A *Markov Chain* is a mathematical model that uses this property to predict the next state of a system given only its current state. Despite its simplicity requiring no memory of past states, this approach can prove powerful across many applications, like natural language processing or predicting how users navigate websites.

## The Basics of a Markov Chain

Before we look at a real-world application of a Markov Chain, let’s familiarize ourselves with some basic concepts. For any stochastic process, we must define the *State Space* S: a complete set of states covering all possible system outcomes. For example, let’s define a system with exactly three states. Then the state space S = {S1, S2, S3}. From any state Sn in S, the system can transition to one of the other two states, or remain in the same state.

![]({{ site.url }}{{ site.baseurl }}/images/MarkovChain/MarkovChainStructure.png)<!-- -->

A transition from state i to state j is written as *i*→*j*. Consider a random state in our system like S1. From S1, there exist three possible transitions:

**S<sub>1</sub>→S<sub>1</sub>** : starting from state S<sub>1</sub>, we stay in state S<sub>1</sub>.  
**S<sub>1</sub>→S<sub>2</sub>** : starting from state S<sub>1</sub>, we transition to state S<sub>2</sub>.  
**S<sub>1</sub>→S<sub>3</sub>** : starting from state S<sub>1</sub>, we transition to state S<sub>3</sub>.  

![]({{ site.url }}{{ site.baseurl }}/images/MarkovChain/TransitionSet.png)<!-- -->

Each transition *i*→*j* has a *transition probability*: the likelihood of transitioning from state i to state j. The transition probability P<sub>ij</sub> can be written as:<br>
<br>
**P<sub>ij</sub> = P(*i*→*j*) = P(S<sub>t+1</sub> = S<sub>j</sub> | S<sub>t</sub> = S<sub>i</sub>) where t represents a step in the system.** where *t* indicates a state-step in the system.<br>

In order to estimate the probability of a transition occuring, P&#770;<sub>ij</sub>, we must first observe the system and count the number of transitions from *i*→*j* that have occurred. These observable counts are denoted c<sub>ij</sub>.

![]({{ site.url }}{{ site.baseurl }}/images/MarkovChain/TransitionCounts.png)<!-- -->

The second requirement is counting the number of transitions from *i*→*k* that have occurred, where *k* denotes any possible transition state from *i*. We can denote the total count of transitions from *i* as c<sub>ik</sub>. Now, we're ready to estimate the transition probability P&#770;<sub>ij</sub>. The maximum likelihood estimator for Markov chain transition probabilities is:
<br>
<br>
**P&#770;<sub>ij</sub> = c<sub>ij</sub> / &sum;<sub>k</sub> c<sub>ik</sub>**

Once calculating P&#770;<sub>ij</sub> for every possible transition in the system, we can update our original diagram with transition probabilities.

![]({{ site.url }}{{ site.baseurl }}/images/MarkovChain/TransitionProbabilities.png)<!-- -->

