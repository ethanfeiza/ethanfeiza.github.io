---
title: "WIP... Exploring Markov Chains"
date: 1999-02-13
tags: [Stochastic Modeling, Linear Algebra, Python]
excerpt: "Explore the Therory of Markov Chains in Stochastic Modeling"
---

## Background
When creating a discrete predictive model, intuition may lead us to believe that more features equals stronger results. A powerful neural network might require an extensive feature set and rich history to predict an accurate output. However, this is not always necessary to achieve the desired result.

A *Stochastic Process* describes a system where state changes are driven by probability. The process follows an assumption known as the *Markov Property*: the next state of the system depends only on the current state, not the sequence of prior states that led to the current. This idea forms a core framework in probabilistic modeling.

A *Markov Chain* is a mathematical model that uses this property to predict the next state of a system given only its current state. Despite its simplicity requiring no memory of past states, this approach can prove powerful across many applications, like natural language processing or predicting how users navigate websites.

## The Basics of a Markov Chain

Before looking at a real-world application of a Markov Chain, let's familiarize ourselves with some basic concepts. For any stochastic process, we must define the *State Space* S: a complete set of states covering all possible system outcomes. As an example, let's define a system with exactly three states. The state space S = {S<sub>1</sub>, S<sub>2</sub>, S<sub>3</sub>}. From any state S<sub>n</sub>, the system can *transition* to one of the other two states, or remain in the same state.

![]({{ site.url }}{{ site.baseurl }}/images/MarkovChain/MarkovChainStructure.png)<!-- -->

A transition from state *i* to state *j* is written as c<sub>ij</sub>. Let's pick a random state in our system like S<sub>1</sub>. From S<sub>1</sub>, there exist three possible transitions:

c<sub>11</sub>: starting from state S<sub>1</sub>, we stay in state S<sub>1</sub>.  
c<sub>12</sub>: starting from state S<sub>1</sub>, we transition to state S<sub>2</sub>.  
c<sub>13</sub>: starting from state S<sub>1</sub>, we transition to state S<sub>3</sub>.  

![]({{ site.url }}{{ site.baseurl }}/images/MarkovChain/TransitionSet.png)<!-- -->

Each transition c<sub>ij</sub> has a transition probability: the likelihood of transitioning from state i to state j. The transition probability c<sub>ij</sub> can be written as: 
 
P( c<sub>ij</sub> ) = P( s<sub>t+1</sub> = *s<sub>j</sub>* | s<sub>t</sub> = *s<sub>i</sub>* ) = P<sub>ij</sub>
